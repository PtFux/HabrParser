# Краулер статей Habr.com

Java-приложение для сбора статей с habr.com, реализующее принципы чистой архитектуры и использующее современные возможности Java.

## Возможности

- Сбор статей с habr.com
- Извлечение заголовка, содержимого, автора, времени публикации и краткого описания статей
- Поддержка сбора с настраиваемыми интервалами
- Хранение статей
- Обработка HTTP-ошибок, следование за редиректом
- Сохранение в эластик, общение между сервисами через очереди в RabbitMQ
- Контейнеризация с Docker - ТОЛЬКО КИБАНА, RABBIT и ELASTIC 
  - Сервисы деплоятся стандартным способом в консоли, докер пока не настроен

!!! Все требования выполняются.

! РИДМИ файлы нуждаются в актуализации

## Архитектура

Приложение имеет модульную структуру со следующими компонентами:

- **MainApp**: Точка входа, инициализирующая и координирующая компоненты
- **Crawler**: Управляет процессом сбора данных и взаимодействием компонентов
- **Parser**: Обрабатывает парсинг HTML и извлечение данных статей
- **Repository**: Управляет хранением статей (реализация в памяти)
- **Scheduler**: Контролирует расписание выполнения сбора
- **UrlGenerator**: Генерирует URL-адреса статей для сбора

## Требования

- Java 11 или выше
- Maven 3.6 или выше
- Docker (опционально, для контейнеризации)

## Сборка приложения

### С использованием Maven

```bash
mvn clean package
```

Собранный JAR-файл будет доступен в `target/habr-crawler-1.0-SNAPSHOT-jar-with-dependencies.jar`

### С использованием Docker

```bash
docker build -t habr-crawler .
```

## Запуск приложения

### С использованием Java

```bash
java -jar target/habr-crawler-1.0-SNAPSHOT-jar-with-dependencies.jar
```

### С использованием Docker

```bash
docker run habr-crawler
```

## Конфигурация

Настройки можно изменить, редактируя параметры в `MainApp.java`:

- `startId`: Начальный ID статьи для сбора
- `endId`: Конечный ID статьи для сбора
- `interval`: Временной интервал между запусками сбора
- `timeUnit`: Единица измерения интервала (HOURS, MINUTES и т.д.)

## Диаграмма архитектуры

```
[MainApp]
    ↓
[CrawlerScheduler]
    ↓
[UrlGenerator] → [HabrCrawler]
                    ↓
                [HabrParser]
                    ↓
                [ArticleRepository]
```

## Обработка ошибок

Реализована обработка:
- Проблем с HTTP-соединением
- Ошибок парсинга HTML
- Некорректных ID статей
- Таймаутов сети

## Логирование

Используется SLF4J с Logback. Логи включают:
- Прогресс сбора данных
- Сообщения об ошибках
- События запуска/остановки приложения
