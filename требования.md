# Требования


## Флоу 
``` 
          [ Config.yaml ] 
          
              ↑| habr.com
              |↓
          
          [ Config.java ] 
          
   loadConfig ↑|  habr.com
              |↓
              
run up -→ [ Main.java ]   -> [ Scheduler.java ]    ->    [ UrlGenerator.java ]   →  [ RabbitMQ ]
                                                            
                             docID | ↑                                                   |  url
             ( hash(title, date) ) ↓ | {docID exist?}                                    ↓ 
                             
                             [ DocRepository ]                                      [HabrCrawler] 
                                                             
                             docID | ↑                                                   |  
             ( hash(title, date) ) ↓ | {docID exist?}                                    ↓ 
                                                   
                             [ Elastic Search ] ← [ ArticleRepository.java ]  ← [ HabrParser.java ]
 
                                                                        
```



## Требования к проекту
### Этап 1. Парсинг

- [ ] Скачивание html/xml страниц
- [ ] Обработка http кодов ошибок
- [ ] Парсинг текста публикации
    - [ ] Идентификатор - хэш заголовка и даты
    - [ ] Заголовок
    - [ ] Время публикации
    - [ ] Автор
    - [ ] Текст публикации
    - [ ] Ссылка на публикацию

### Этап 2. Использование очередей

- [ ] Использование RabbitMQ
- [ ] Очередь с задачами на парсинг (Ссылки со стартовой страницы)
- [ ] Очередь с результатами парсинга
- [ ] Защита от потери необработанных сообщений (флаг Ack)
- [ ] Использование Basic.Consume и Basic.Get, описать разницу

### Этап 3. Сохранение в базу данных

- [ ] Использование ElasticSearch
- [ ] Использование индекса для хранения документов
- [ ] В планировщике (при парсинге стартовой страницы) сохранять с идентификаторами == хэш заголовка и даты документа
- [ ] Проверять наличие в базе документа по идентификатору (хеш) - не пускать в обработку если уже существует
- [ ] Считывать документы из очереди с результатами и обновлять их в базе по идентификатору (хэш от заголовка и даты документа)


### Этап 4. Скрипты для работы с Elastic Search
- [ ] Запрос на поиск по нескольким полям документа в разных комбинациях (с различными логическими операторами)
- [ ] Подготовить сложные полнотекстовые поисковые запросы (понимать синтаксис полнотекстового поиска ElasticSearch)
- [ ] Запросы на различные типы агрегаций (например вывод гистограммы по количеству публикаций на различные даты и для различных авторов)
